{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "# from fastai.text.all import *\n",
    "# from fastai.text.all import get_text_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.4.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights = 'gpt2'\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(pretrained_weights)\n",
    "model = GPT2LMHeadModel.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'in' in words.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_train_path = '/home/advaitmb/.fastai/data/imdb/train/pos/'\n",
    "positive_train_files = get_text_files(positive_train_path)\n",
    "\n",
    "negative_train_path = '/home/advaitmb/.fastai/data/imdb/train/neg/'\n",
    "negative_train_files = get_text_files(negative_train_path)\n",
    "\n",
    "\n",
    "positive_test_path = '/home/advaitmb/.fastai/data/imdb/test/pos/'\n",
    "positive_test_files = get_text_files(positive_test_path)\n",
    "\n",
    "negative_test_path = '/home/advaitmb/.fastai/data/imdb/test/neg/'\n",
    "negative_test_files = get_text_files(negative_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def files_to_list(files):\n",
    "    ls = []\n",
    "    for file in files:\n",
    "        with open(file, 'r') as f:\n",
    "            ls.append(f.read())\n",
    "    return ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_train_text_list = files_to_list(positive_train_files)\n",
    "negative_train_text_list = files_to_list(negative_train_files)\n",
    "positive_test_text_list = files_to_list(positive_test_files)\n",
    "negative_test_text_list = files_to_list(negative_test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_html(raw_html):\n",
    "    cleanr = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_newlines(raw_text):\n",
    "    return raw_text.replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(positive_train_text_list)):\n",
    "    positive_train_text_list[i] = clean_newlines(clean_html(positive_train_text_list[i]))\n",
    "    \n",
    "for i in range(len(negative_train_text_list)):\n",
    "    negative_train_text_list[i] = clean_newlines(clean_html(negative_train_text_list[i]))\n",
    "    \n",
    "for i in range(len(positive_test_text_list)):\n",
    "    positive_test_text_list[i] = clean_newlines(clean_html(positive_test_text_list[i]))\n",
    "    \n",
    "for i in range(len(negative_test_text_list)):\n",
    "    negative_test_text_list[i] = clean_newlines(clean_html(negative_test_text_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('positive_train_text', 'w') as f:\n",
    "    for text in positive_train_text_list:\n",
    "        f.write(text + '\\n')\n",
    "        \n",
    "with open('negative_train_text', 'w') as f:\n",
    "    for text in negative_train_text_list:\n",
    "        f.write(text + '\\n')\n",
    "        \n",
    "with open('positive_test_text', 'w') as f:\n",
    "    for text in positive_test_text_list:\n",
    "        f.write(text + '\\n')\n",
    "        \n",
    "with open('negative_test_text', 'w') as f:\n",
    "    for text in negative_test_text_list:\n",
    "        f.write(text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steven Speilberg's adaptation of Alice Walkers popular novel is not without its share of controversy. When first released members of the black community criticised its treatment of black men, while others questioned why a white man was directing this film about black women.This is the story of a young black woman named Celie, growing up in rural America after the turn of the century. She has two children by her abusive father which are snatched from her arms at birth. Her only solace in her miserable life comes from her sister.Celie (played in later years by newcomer Whoopie Goldberg) is married off to an abusive husband (Danny Glover). The husband is humiliated by the sister and so she is quickly removed from Celie's life.The story is often heartbreaking as Celie keeps up hope that she may one day be reunited with her sister and with her children. Throughout her life she meets an assortment of characters, including Sophia, a tough as nails wife to her step son, and Shug, a loud and luscious saloon singer, who teaches her a thing or two about love.Speilberg's direction is all over this picture, which offers brilliant cinematography and some stellar performances. I dare you to watch this film and not be moved! The film The Color Purple manages to capture the essence of what is a complicated story. While it tends to minimise the lesbian aspects as well as the African story, both of which were so vivid in the book, the movie remains true to its themes, allowing the voice of Alice Walker to shine through.I couldn't begin to respond to the controversy that surrounded this film. Suffice it to say, however, this is one of the few films that I can watch again and again, and which has left an indelible mark on me.\n",
      "I first saw this film when it aired on the now defunct Trio Channel a few years ago, and recently watched it again--sans commercials--on Sundance. I was impressed the first time, and found it even more engaging on second viewing. Yes, some of the segments are far from perfect--Amos Gitai's hysterical commentary stands out like a sore thumb--but taken collectively, 11 09 01 is a total success. Best of show: Shohei Imamura's amazing final segment, which contemporary critics such as the thick-witted Mick LaSalle somehow misinterpreted as an attack on 'the terrorists', but now stands revealed as a masterful anti-war polemic; Samira Makhmalbaf's opening piece that manages to blend deep empathy for the victims of 9/11 with a prescient concern for the children of Afghanistan; and Idrissa Ouedraogo's amusing children's crusade for Osama Bin Laden--a hunt almost as serious and successful an undertaking as the one for the REAL Osama. Youssef Chahine's segment is a noble if failed experiment which at least has the guts to remind the audience that Bin Laden and al'Qaeda are basically creations of American foreign policy and the CIA, and though Sean Penn's character study seems out of place, it's still an effectively bittersweet piece of film-making. All in all, essential viewing, and a darn sight better than Oliver Stone's reactionary World Trade Center.\n",
      "This movie is beautifully designed! There are no flaws. Not in the design of the set, the lighting, the sounds, the plot. The script is an invitation to a complex game where the participants are on a simple mission.Paxton is at his best in this role. His mannerisms, the infections used in the tones of his voice are without miscue. Each shot meticulously done! Surprises turn up one after another when the movie reaches past its first hour. This may not be the best picture of the year, but it's a gem that has been very well polished. It's not for the simple mind.\n",
      "Like his earlier film, \"In a Glass Cage\", Agust√≠ Villaronga achieves an intense and highly poetic canvas that is even more refined visually than its predecessor. This is one of the most visually accomplished and haunting pictures one could ever see. The heightened drama, intensity and undertone of violence threatens on the the melodramatic or farcical, yet never steps into it. In that way, it pulls off an almost impossible feat: to be so over-the-top and yet so painfully restrained, to be so charged and yet so understated, and even the explosives finales are virtuosic feasts of the eye. Unabashed, gorgeous, and highly tense... this film is simply superb!\n",
      "A famous orchestra conductor, Daniel Dareus, suffers what appears a heart attack as he finished conducting a concert. Suddenly, we watch him as he arrives in the small town that he has left years before. Since he left so young, and having his name changed contributes to give him a new persona. He has bought the old school building where he plans to stay. The building needs a lot of work. One would expect a man in his position to have all the comforts of the world he left behind to be installed in his new abode, but no, Daniel puts up with the harsh winter in his own way.The local pastor, Stig, whose church has a small choir, comes calling to see if he can intere\n"
     ]
    }
   ],
   "source": [
    "with open('positive_train_text', 'r') as f:\n",
    "    print(f.read()[:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'negative_train_text'\n",
    "test_path = 'negative_test_text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextDataset,DataCollatorForLanguageModeling\n",
    "\n",
    "def load_dataset(train_path,test_path,tokenizer):\n",
    "    train_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=train_path,\n",
    "          block_size=128)\n",
    "     \n",
    "    test_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=test_path,\n",
    "          block_size=128)   \n",
    "    \n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False,\n",
    "    )\n",
    "    return train_dataset,test_dataset,data_collator\n",
    "\n",
    "train_dataset,test_dataset,data_collator = load_dataset(train_path,test_path,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-imdb-negative-sentiment\", #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=3, # number of training epochs\n",
    "    per_device_train_batch_size=8, # batch size for training\n",
    "    per_device_eval_batch_size=8,  # batch size for evaluation\n",
    "    eval_steps = 400, # Number of update steps between two evaluations.\n",
    "    save_steps=800, # after # steps model is saved \n",
    "    warmup_steps=500,# number of warmup steps for learning rate scheduler\n",
    "    )\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    prediction_loss_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='10485' max='10485' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10485/10485 1:51:23, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.020727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.946644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.902403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.869069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>3.872636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.834459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>3.822205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>3.684422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>3.703885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>3.686973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>3.684258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>3.690805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>3.675539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>3.675410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>3.588566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>3.584961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>3.597270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>3.601824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>3.588508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>3.588297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10485, training_loss=3.724238122913686)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"acomplete\"\n",
    "prompt_ids = tokenizer.encode(prompt)\n",
    "inp = tensor(prompt_ids)[None].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 330,  296, 6677]], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    }
   ],
   "source": [
    "beam_outputs = trainer.model.generate(\n",
    "    inp, \n",
    "    max_length=20, \n",
    "    num_beams=10, \n",
    "    repeatition_penalty=2.,  \n",
    "    early_stopping=True,\n",
    "    do_sample=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: \n",
      "this movie is the worst movie I have ever seen in my life. The acting is terrible, the\n",
      "1: \n",
      "this movie is a waste of your time and money. If you want to see a good movie,\n",
      "2: \n",
      "this movie is so bad it's good, but it's not even good enough to be good.\n",
      "3: \n",
      "this movie is so bad, it's hard to believe that it was made in the first place.\n",
      "4: \n",
      "this movie is so bad, it's hard to believe that it was made in the first place.\n",
      "5: \n",
      "this movie is a waste of time and money.\n",
      "This movie is a waste of time and money\n",
      "6: \n",
      "this movie is so bad that it's not even funny. The acting is so bad that it's\n",
      "7: \n",
      "this movie is a waste of time and money.\n",
      "This is the worst movie I have ever seen\n",
      "8: \n",
      "this movie is a waste of time. If you want to watch a bad movie, don't watch\n",
      "9: \n",
      "this movie is a waste of time. It's not funny, it's not funny, it's\n"
     ]
    }
   ],
   "source": [
    "for i, beam_output in enumerate(beam_outputs):\n",
    "    output = tokenizer.decode(beam_output.cpu().numpy(), skip_special_tokens=True)\n",
    "#     sentiment = sentiment_model.forward(sentiment_tokenizer.encode(prompt[len(prompt):], return_tensors=\"pt\"))\n",
    "    \n",
    "    print(\"{}: \\n{}\".format(i, output))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai2",
   "language": "python",
   "name": "fastai2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
